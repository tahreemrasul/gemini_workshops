{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyO2THZCXyS3ypzMqCjEcc5c",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/tahreemrasul/gemini_workshops/blob/main/simple_agent_from_scratch.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Build an agent from scratch using Gemini\n",
        "\n",
        "<table align=\"left\">\n",
        "  <td style=\"text-align: center\">\n",
        "    <a href=\"https://colab.research.google.com/github/tahreemrasul/gemini_workshops/blob/main/simple_agent_from_scratch.ipynb\">\n",
        "      <img width=\"32px\" src=\"https://www.gstatic.com/pantheon/images/bigquery/welcome_page/colab-logo.svg\" alt=\"Google Colaboratory logo\"><br> Open in Colab\n",
        "    </a>\n",
        "  </td>\n",
        "  <td style=\"text-align: center\">\n",
        "    <a href=\"https://github.com/tahreemrasul/gemini_workshops/blob/main/simple_agent_from_scratch.ipynb\">\n",
        "      <img width=\"32px\" src=\"https://raw.githubusercontent.com/primer/octicons/refs/heads/main/icons/mark-github-24.svg\" alt=\"GitHub logo\"><br> View on GitHub\n",
        "    </a>\n",
        "  </td>\n",
        "</table>\n",
        "\n",
        "<div style=\"clear: both;\"></div>    "
      ],
      "metadata": {
        "id": "KLHiwA9_OSEH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "| Author |\n",
        "| --- |\n",
        "| [Tahreem Rasul](https://github.com/tahreemrasul/) |"
      ],
      "metadata": {
        "id": "U6_CYm8PONiB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This notebook shows how to build a simple agent using the Gemini API and the\n",
        "[Google GenAI Python SDK](https://pypi.org/project/google-genai/).\n",
        "\n",
        "At its core, an agent is just:\n",
        "\n",
        "1. An LLM (Gemini).\n",
        "2. A set of tools (Python functions the model can call).\n",
        "3. A loop that lets the model **think â†’ act â†’ observe â†’ continue thinking**.\n",
        "\n",
        "Weâ€™ll:\n",
        "\n",
        "- Define a tiny `get_weather` tool.\n",
        "- Let Gemini decide when to call it (via function calling).\n",
        "- Wrap that into a minimal `while True` loop to make it *agentic*."
      ],
      "metadata": {
        "id": "aWARZ7dVOLtC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Getting Started"
      ],
      "metadata": {
        "id": "1qj-BxQDTagz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Install & Import Dependencies"
      ],
      "metadata": {
        "id": "O892najYPZqL"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yQ9LZeVZOCuZ"
      },
      "outputs": [],
      "source": [
        "!pip install -U -q google-genai"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import getpass\n",
        "from typing import Dict\n",
        "\n",
        "from google import genai\n",
        "from google.genai import types\n",
        "\n",
        "print(\"google-genai version:\", genai.__version__)"
      ],
      "metadata": {
        "id": "thRMSX5BPf79"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dmWOrTJ3gx13"
      },
      "source": [
        "### Authenticate your notebook environment (Colab only)\n",
        "\n",
        "If you are running this notebook on Google Colab, run the following cell to authenticate your environment."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NyKGtVQjgx13"
      },
      "outputs": [],
      "source": [
        "import sys\n",
        "\n",
        "if \"google.colab\" in sys.modules:\n",
        "    from google.colab import auth\n",
        "\n",
        "    auth.authenticate_user()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Get API Key\n",
        "\n",
        "There are a few different ways of accessing the Gemini models in your applications. Most notably, you can do this through Vertex AI in Google Cloud Platform, or via the API method using Google AI Studio. We will be using the second approach.\n",
        "\n",
        "Head over to https://aistudio.google.com/ and generate an API Key.\n",
        "1. Click on **API Keys** on the left sidebar.\n",
        "2. You should now see an option to **Create API Key**. Click on this.\n",
        "3. A dialog box will open. You will be prompted to enter the name of the key. Enter a name of your choice. You also need to link this to a Google Cloud Project. From the dropdown of project selection, select **Create project**. You will be prompted to name the project, choose a name of your choice. Click on **Create Key**.\n",
        "4. You should now see your created API key in the list. Click on the key and copy it. Paste it in the cell below.\n"
      ],
      "metadata": {
        "id": "haJFf24ZxpS9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ðŸ”‘ Paste your API key from Google AI Studio\n",
        "API_KEY = \"your-api-key\"\n",
        "\n",
        "from google import genai\n",
        "client = genai.Client(api_key=API_KEY)"
      ],
      "metadata": {
        "id": "Eop8m3uZzZHK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5671450907ec"
      },
      "source": [
        "### Choose a model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "41e499d90618"
      },
      "outputs": [],
      "source": [
        "MODEL_ID = \"gemini-2.5-flash\"  # @param {type: \"string\"}"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Build the agent"
      ],
      "metadata": {
        "id": "7zFXeOerTf2d"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Step 1 - Define a simple weather tool\n",
        "\n",
        "\n",
        "We'll start with a mock weather function:\n",
        "\n",
        "- In a real app, this would call a live weather API.\n",
        "- In this notebook, it just returns hard-coded values for a few cities.\n",
        "\n",
        "Then we describe it to Gemini using a **FunctionDeclaration** + **Tool**, so the\n",
        "model knows:\n",
        "\n",
        "- the tool name (`get_weather`)\n",
        "- what it does\n",
        "- what parameters it expects (`city: string`)"
      ],
      "metadata": {
        "id": "pjQAwLK2QlBW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_weather_impl(city: str) -> Dict:\n",
        "    \"\"\"Mock weather function.\n",
        "\n",
        "    In a real app, this would call an external API.\n",
        "    Here we just return fake but structured data.\n",
        "    \"\"\"\n",
        "    fake_temps = {\n",
        "        \"berlin\": {\"temperature_c\": 17, \"condition\": \"cloudy\"},\n",
        "        \"london\": {\"temperature_c\": 15, \"condition\": \"windy\"},\n",
        "        \"lahore\": {\"temperature_c\": 32, \"condition\": \"hot\"},\n",
        "        \"islamabad\": {\"temperature_c\": 28, \"condition\": \"warm and clear\"},\n",
        "        \"paris\": {\"temperature_c\": 19, \"condition\": \"light rain\"},\n",
        "    }\n",
        "\n",
        "    city_key = city.lower()\n",
        "    # return default fallback\n",
        "    data = fake_temps.get(\n",
        "        city_key,\n",
        "        {\"temperature_c\": 22, \"condition\": \"mild and unknown\"},\n",
        "    )\n",
        "    return {\n",
        "        \"city\": city,\n",
        "        \"temperature_c\": data[\"temperature_c\"],\n",
        "        \"condition\": data[\"condition\"],\n",
        "    }"
      ],
      "metadata": {
        "id": "g4JIWKibQbFP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We will now describe the function to the model."
      ],
      "metadata": {
        "id": "1fic2FEVQ_Lp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "get_weather_decl = types.FunctionDeclaration(\n",
        "    name=\"get_weather\",\n",
        "    description=\"Get the current (mock) weather for a given city.\",\n",
        "    parameters={\n",
        "        \"type\": \"object\",\n",
        "        \"properties\": {\n",
        "            \"city\": {\n",
        "                \"type\": \"string\",\n",
        "                \"description\": \"City name, e.g. 'London' or 'Lahore'.\",\n",
        "            }\n",
        "        },\n",
        "        \"required\": [\"city\"],\n",
        "    },\n",
        ")"
      ],
      "metadata": {
        "id": "juIxPfwtRAOO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The final step would be to wrap this function as a tool."
      ],
      "metadata": {
        "id": "OxI_RTkwRGUo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "weather_tool = types.Tool(function_declarations=[get_weather_decl])\n",
        "\n",
        "print(weather_tool)"
      ],
      "metadata": {
        "id": "lB4SHn01RM41"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Step 2 â€” Make a single call with tools\n",
        "\n",
        "Before we build a loop, let's:\n",
        "\n",
        "1. Send a user query.\n",
        "2. Give Gemini our `weather_tool`.\n",
        "3. Inspect whether the model **requests a function call**.\n",
        "\n",
        "We'll deliberately not let the SDK auto-handle function calls; instead,\n",
        "we'll inspect `response.function_calls` ourselves so the agent loop is explicit."
      ],
      "metadata": {
        "id": "2L2W53DiRQGs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def call_model_once(user_input: str):\n",
        "    \"\"\"Send one message to the model with our weather tool attached.\"\"\"\n",
        "    response = client.models.generate_content(\n",
        "        model=MODEL_ID,\n",
        "        contents=[user_input],\n",
        "        config=types.GenerateContentConfig(\n",
        "            tools=[weather_tool],\n",
        "            # optional: temperature=0.2, etc.\n",
        "        ),\n",
        "    )\n",
        "    return response"
      ],
      "metadata": {
        "id": "7JRTe6kaRPCt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_response = call_model_once(\"What's the weather like in Islamabad right now?\")\n",
        "print(\"\\nFunction calls:\", test_response.function_calls)"
      ],
      "metadata": {
        "id": "W_Tps57_RdeJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Step 3 â€” Build the minimal agent loop\n",
        "\n",
        "Now we'll create a loop:\n",
        "\n",
        "1. Read user input.\n",
        "2. Ask the model what to do (with tools available).\n",
        "3. **If** the model asks to call `get_weather`, we:\n",
        "   - parse the tool arguments,\n",
        "   - run `get_weather_impl(...)`,\n",
        "   - send the result back to the model as a *tool response*.\n",
        "4. Print the model's final answer.\n",
        "5. Repeat.\n",
        "\n",
        "This is the essence of an agent:\n",
        "\n",
        "> **reason â†’ act (tool) â†’ observe (tool result) â†’ continue reasoning**\n"
      ],
      "metadata": {
        "id": "Js93YEK0R94I"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def run_minimal_agent():\n",
        "    print(\"Gemini agent. Type 'exit' or 'quit' to stop.\\n\")\n",
        "\n",
        "    while True:\n",
        "        user_input = input(\"You: \")\n",
        "        if user_input.strip().lower() in {\"exit\", \"quit\", \"q\"}:\n",
        "            print(\"ðŸ‘‹ Bye!\")\n",
        "            break\n",
        "\n",
        "        # 1) First turn: ask the model what to do, with tools available\n",
        "        response = client.models.generate_content(\n",
        "            model=MODEL_ID,\n",
        "            contents=[user_input],\n",
        "            config=types.GenerateContentConfig(\n",
        "                tools=[weather_tool],\n",
        "            ),\n",
        "        )\n",
        "\n",
        "        # 2) If the model decided to call a tool, handle it\n",
        "        if response.function_calls:\n",
        "            tool_call = response.function_calls[0]\n",
        "            print(f\"[debug] Model wants to call: {tool_call.name} with args {tool_call.args}\")\n",
        "\n",
        "            if tool_call.name == \"get_weather\":\n",
        "                # Extract arguments from the function call\n",
        "                city = tool_call.args.get(\"city\", \"\")\n",
        "                tool_result = get_weather_impl(city=city)\n",
        "\n",
        "                # Wrap the tool result as a function response part\n",
        "                function_response_part = types.Part.from_function_response(\n",
        "                    name=tool_call.name,\n",
        "                    response=tool_result,\n",
        "                )\n",
        "\n",
        "                # 3) Second turn: send user + tool result back to the model\n",
        "                response = client.models.generate_content(\n",
        "                    model=MODEL_ID,\n",
        "                    contents=[\n",
        "                        user_input,              # original user message\n",
        "                        function_response_part,  # tool response\n",
        "                    ],\n",
        "                    config=types.GenerateContentConfig(\n",
        "                        tools=[weather_tool],\n",
        "                    ),\n",
        "                )\n",
        "\n",
        "        # 4) Print the final answer\n",
        "        print(\"Agent:\", response.text)\n",
        "        print(\"-\" * 60)\n",
        "\n",
        "run_minimal_agent()"
      ],
      "metadata": {
        "id": "v32jJOZsSFhh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Step 4 â€” Play with it: make it feel more \"agentic\"\n",
        "\n",
        "Some ideas to extend this notebook:\n",
        "\n",
        "1. **Add a second tool**  \n",
        "   For example: `convert_temperature` that converts Â°C â†” Â°F.\n",
        "   - Declare it as another `FunctionDeclaration`.\n",
        "   - Add it to the same `Tool` or another tool in `GenerateContentConfig`.\n",
        "   - Update the loop to handle calls to either `get_weather` or `convert_temperature`.\n",
        "\n",
        "2. **Add simple memory**  \n",
        "   Keep a `history` list of previous turns:\n",
        "   ```python\n",
        "   history = []\n",
        "   # each loop:\n",
        "   history.append({\"role\": \"user\", \"content\": user_input})\n",
        "   history.append({\"role\": \"agent\", \"content\": response.text})\n"
      ],
      "metadata": {
        "id": "XyLdN8UOTKCo"
      }
    }
  ]
}